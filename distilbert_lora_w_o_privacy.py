# -*- coding: utf-8 -*-
"""distilbert-lora-w/o-privacy.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1V7u9PZQyb5J8bUVRj5DLzTKpKXl_DwdG
"""

pip install transformers datasets torch peft

import torch
import torch.nn as nn
import torch.optim as optim
from transformers import DistilBertForSequenceClassification, DistilBertTokenizer
from transformers import get_linear_schedule_with_warmup
from torch.utils.data import DataLoader, Dataset
from torch.optim import AdamW
import math
from typing import List, Dict, Optional
import numpy as np
import pandas as pd
import nltk
from nltk.stem import PorterStemmer
from nltk.corpus import stopwords
from sklearn.metrics import accuracy_score, classification_report
from sklearn.model_selection import train_test_split
from tqdm import tqdm
from peft import get_peft_model, LoraConfig, TaskType

splits = {'train': 'yelp_review_full/train-00000-of-00001.parquet', 'test': 'yelp_review_full/test-00000-of-00001.parquet'}
df = pd.read_parquet("hf://datasets/Yelp/yelp_review_full/" + splits["train"])
df.head()

df.isnull().sum()

df.dropna(inplace=True)
df = df.iloc[:100001,:]
df.shape

"""# Data Prep"""

# converting text to lower case
# df = df.copy()
df = df.applymap(lambda x: x.lower() if isinstance(x, str) else x)

# removing contractions
contractions = {
    "ain't": "is not", "aren't": "are not", "can't": "cannot", "couldn't": "could not", "didn't": "did not",
    "doesn't": "does not", "don't": "do not", "hadn't": "had not", "hasn't": "has not", "haven't": "have not",
    "he'd": "he would", "he'll": "he will", "he's": "he is", "how'd": "how did", "how'll": "how will",
    "how's": "how is", "I’d": "I would", "I'll": "I will", "I'm": "I am", "I've": "I have", "I’m": "I am",
    "isn't": "is not", "it'd": "it would", "it'll": "it will", "it's": "it is", "let's": "let us", "ma'am": "madam",
    "mightn't": "might not", "mustn't": "must not", "needn't": "need not", "shan't": "shall not", "she'd": "she would",
    "she'll": "she will", "she's": "she is", "shouldn't": "should not", "that'd": "that would", "that'll": "that will",
    "that's": "that is", "there'd": "there would", "there'll": "there will", "there's": "there is", "they'd": "they would",
    "they'll": "they will", "they're": "they are", "they've": "they have", "wasn't": "was not", "weren't": "were not",
    "what'll": "what will", "what's": "what is", "what've": "what have", "where'd": "where did", "where'll": "where will",
    "where's": "where is", "who'd": "who would", "who'll": "who will", "who's": "who is", "who've": "who have",
    "why'd": "why did", "why'll": "why will", "why's": "why is", "won't": "will not", "wouldn't": "would not",
}
def expand_contractions(text):
    # Check if the input is a float
    if isinstance(text, float):
        return text

    for contraction, expanded in contractions.items():
        text = text.replace(contraction, expanded)

    return text

# apply
df['text'] = df['text'].apply(expand_contractions)

# removing puntuations
df = df.replace(r'[^\w\s]', '', regex=True)

# stop words removal

nltk.download('stopwords')
stop = set(stopwords.words('english'))

# for all col
df['text'] = df['text'].apply(lambda x: ' '.join([word for word in str(x).split() if word not in stop]) if x is not None else '')

# stemming
nltk.download('punkt_tab')
stemmer = PorterStemmer()

def stem_text(text):
    if isinstance(text, str):
        words = nltk.word_tokenize(text)
        stemmed_words = [stemmer.stem(word) for word in words]
        return ' '.join(stemmed_words)


df['text'] = df['text'].apply(stem_text)

df.head(5)

df.shape

"""# BERT"""

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from transformers import BertTokenizer
from torch.utils.data import Dataset, DataLoader
import torch
from transformers import BertForSequenceClassification #AdamW
from torch.nn import CrossEntropyLoss

# Encoding the target variable 'Popularity'
le = LabelEncoder()
df['label'] = le.fit_transform(df['label'])

from sklearn.model_selection import train_test_split

# Split data (80% train, 20% test)
X_train, X_test, y_train, y_test = train_test_split(
    df['text'], df['label'], test_size=0.2, random_state=42
)

from transformers import BertTokenizer

# Load the BERT tokenizer
tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')

# Define a function to tokenize the text data
def tokenize_texts(texts, max_length=512):
    return tokenizer.batch_encode_plus(
        texts,
        max_length=max_length,
        padding='max_length',
        truncation=True,
        return_tensors='pt'  # Return PyTorch tensors
    )

# Tokenize the training and test data
train_encodings = tokenize_texts(X_train.tolist())
test_encodings = tokenize_texts(X_test.tolist())

torch.save(train_encodings, 'train_encodings.pt')
torch.save(test_encodings, 'test_encodings.pt')

train_encodings = torch.load('train_encodings.pt', weights_only=False)
test_encodings = torch.load('test_encodings.pt', weights_only=False)

y_train_tensor = torch.tensor(y_train.tolist(), dtype=torch.long)
y_test_tensor = torch.tensor(y_test.tolist(), dtype=torch.long)

class TextDataset(torch.utils.data.Dataset):
    def __init__(self, encodings, labels):
        self.encodings = encodings
        self.labels = labels

    def __len__(self):
        return len(self.labels)

    def __getitem__(self, idx):
        return {
            "input_ids": self.encodings["input_ids"][idx],
            "attention_mask": self.encodings["attention_mask"][idx],
            "labels": self.labels[idx]
        }

# Create Dataset objects
train_dataset = TextDataset(train_encodings, y_train_tensor)
test_dataset = TextDataset(test_encodings, y_test_tensor)

BATCH_SIZE = 32
# DataLoader
train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)
test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)

model = DistilBertForSequenceClassification.from_pretrained(
    'distilbert-base-uncased',
    num_labels=5  # 5 class labels
)

lora_config = LoraConfig(
    r=8,  # Rank
    lora_alpha=32,
    target_modules=["q_lin", "v_lin"],  # BERT attention modules to modify
    lora_dropout=0.1,
    bias="none",
    task_type=TaskType.SEQ_CLS
)

model = get_peft_model(model, lora_config)

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
model.to(device)
print("Available Device: ", device)

loss_fn = torch.nn.CrossEntropyLoss()

optimizer = AdamW(model.parameters(), lr=2e-5)
epochs = 3

for epoch in range(epochs):
    model.train()
    train_losses = []
    total_loss = 0.0

    for batch in tqdm(train_loader):
        optimizer.zero_grad()

        input_ids = batch['input_ids'].to(device)
        attention_mask = batch['attention_mask'].to(device)
        labels = batch['labels'].to(device)

        outputs = model(
            input_ids=input_ids,
            attention_mask=attention_mask
        )

        # Using CrossEntropyLoss
        loss = loss_fn(outputs.logits, labels)
        train_losses.append(loss.item())

        loss.backward()
        optimizer.step()

        total_loss += loss.item()

    total_steps = len(train_loader) * epochs
    scheduler = get_linear_schedule_with_warmup(optimizer,
                                             num_warmup_steps=0,
                                             num_training_steps=total_steps)
    scheduler.step()

    avg_train_loss = total_loss / len(train_loader)
    print(f"Training Loss: {avg_train_loss:.4f}")

model.eval()
test_preds = []
test_true = []

with torch.no_grad():
    for batch in tqdm(test_loader):
        input_ids = batch['input_ids'].to(device)
        attention_mask = batch['attention_mask'].to(device)
        labels = batch['labels'].to(device)

        outputs = model(
            input_ids=input_ids,
            attention_mask=attention_mask
        )

        # Get predicted class (argmax)
        preds = torch.argmax(outputs.logits, dim=1)
        test_preds.extend(preds.cpu().numpy())
        test_true.extend(labels.cpu().numpy())

test_accuracy = accuracy_score(test_true, test_preds)

all_preds = np.array(test_preds)
all_labels = np.array(test_true)

all_labels_indices = np.argmax(all_labels, axis=1) if all_labels.ndim > 1 else all_labels
all_preds_indices = np.argmax(all_preds, axis=1) if all_preds.ndim > 1 else all_preds

# Now compute F1 score
from sklearn.metrics import f1_score, confusion_matrix

f1 = f1_score(all_labels_indices, all_preds_indices, average='weighted')
print(f"F1 Score: {f1:.4f}")

conf_matrix = confusion_matrix(all_labels_indices, all_preds_indices)
print("Confusion Matrix:\n", conf_matrix)

print(f'Epoch {epoch+1}/{epochs}:')
print(f'  Train Loss: {np.mean(train_losses):.4f}')
print(f'  Test Accuracy: {test_accuracy:.4f}')
print(classification_report(test_true, test_preds))

torch.save(model.state_dict(), 'distilbert_lora.pt')

