# -*- coding: utf-8 -*-
"""bert-reatt-privacy.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1eaBkLElFOA3w1vmPxOsxyLKQGVwpKWsI
"""

pip install transformers datasets torch peft opacus

import torch
import torch.nn as nn
import torch.optim as optim
from transformers import BertForSequenceClassification, BertTokenizer, BertModel, BertConfig
from transformers import get_linear_schedule_with_warmup
from torch.utils.data import DataLoader, Dataset
from torch.optim import AdamW, Optimizer
import math
from typing import List, Dict, Optional, Tuple, Union
import numpy as np
import pandas as pd
import nltk
from nltk.stem import PorterStemmer
from nltk.corpus import stopwords
from sklearn.metrics import accuracy_score, classification_report, f1_score, confusion_matrix
from sklearn.model_selection import train_test_split
from tqdm import tqdm
from peft import get_peft_model, LoraConfig, TaskType
import matplotlib.pyplot as plt
import types
import copy

splits = {'train': 'yelp_review_full/train-00000-of-00001.parquet', 'test': 'yelp_review_full/test-00000-of-00001.parquet'}
df = pd.read_parquet("hf://datasets/Yelp/yelp_review_full/" + splits["train"])
df.head()

df.isnull().sum()

df.dropna(inplace=True)
df = df.iloc[:100001,:]
df.shape

"""# Data Prep"""

# converting text to lower case
# df = df.copy()
df = df.applymap(lambda x: x.lower() if isinstance(x, str) else x)

# removing contractions
contractions = {
    "ain't": "is not", "aren't": "are not", "can't": "cannot", "couldn't": "could not", "didn't": "did not",
    "doesn't": "does not", "don't": "do not", "hadn't": "had not", "hasn't": "has not", "haven't": "have not",
    "he'd": "he would", "he'll": "he will", "he's": "he is", "how'd": "how did", "how'll": "how will",
    "how's": "how is", "I’d": "I would", "I'll": "I will", "I'm": "I am", "I've": "I have", "I’m": "I am",
    "isn't": "is not", "it'd": "it would", "it'll": "it will", "it's": "it is", "let's": "let us", "ma'am": "madam",
    "mightn't": "might not", "mustn't": "must not", "needn't": "need not", "shan't": "shall not", "she'd": "she would",
    "she'll": "she will", "she's": "she is", "shouldn't": "should not", "that'd": "that would", "that'll": "that will",
    "that's": "that is", "there'd": "there would", "there'll": "there will", "there's": "there is", "they'd": "they would",
    "they'll": "they will", "they're": "they are", "they've": "they have", "wasn't": "was not", "weren't": "were not",
    "what'll": "what will", "what's": "what is", "what've": "what have", "where'd": "where did", "where'll": "where will",
    "where's": "where is", "who'd": "who would", "who'll": "who will", "who's": "who is", "who've": "who have",
    "why'd": "why did", "why'll": "why will", "why's": "why is", "won't": "will not", "wouldn't": "would not",
}
def expand_contractions(text):
    # Check if the input is a float
    if isinstance(text, float):
        return text

    for contraction, expanded in contractions.items():
        text = text.replace(contraction, expanded)

    return text

# apply
df['text'] = df['text'].apply(expand_contractions)

# removing puntuations
df = df.replace(r'[^\w\s]', '', regex=True)

# stop words removal

nltk.download('stopwords')
stop = set(stopwords.words('english'))

# for all col
df['text'] = df['text'].apply(lambda x: ' '.join([word for word in str(x).split() if word not in stop]) if x is not None else '')

# stemming
nltk.download('punkt_tab')
stemmer = PorterStemmer()

def stem_text(text):
    if isinstance(text, str):
        words = nltk.word_tokenize(text)
        stemmed_words = [stemmer.stem(word) for word in words]
        return ' '.join(stemmed_words)


df['text'] = df['text'].apply(stem_text)

df.head(5)

df.shape

"""# BERT"""

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from transformers import BertTokenizer
from torch.utils.data import Dataset, DataLoader
import torch
from transformers import BertForSequenceClassification #AdamW
from torch.nn import CrossEntropyLoss

# Encoding the target variable 'Popularity'
le = LabelEncoder()
df['label'] = le.fit_transform(df['label'])

from sklearn.model_selection import train_test_split

# Split data (80% train, 20% test)
X_train, X_test, y_train, y_test = train_test_split(
    df['text'], df['label'], test_size=0.2, random_state=42
)

from transformers import BertTokenizer

# Load the BERT tokenizer
tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')

# Define a function to tokenize the text data
def tokenize_texts(texts, max_length=512):
    return tokenizer.batch_encode_plus(
        texts,
        max_length=max_length,
        padding='max_length',
        truncation=True,
        return_tensors='pt'  # Return PyTorch tensors
    )

# Tokenize the training and test data
train_encodings = tokenize_texts(X_train.tolist())
test_encodings = tokenize_texts(X_test.tolist())

torch.save(train_encodings, 'train_encodings.pt')
torch.save(test_encodings, 'test_encodings.pt')

train_encodings = torch.load('train_encodings.pt', weights_only=False)
test_encodings = torch.load('test_encodings.pt', weights_only=False)

y_train_tensor = torch.tensor(y_train.tolist(), dtype=torch.long)
y_test_tensor = torch.tensor(y_test.tolist(), dtype=torch.long)

class TextDataset(torch.utils.data.Dataset):
    def __init__(self, encodings, labels):
        self.encodings = encodings
        self.labels = labels

    def __len__(self):
        return len(self.labels)

    def __getitem__(self, idx):
        return {
            "input_ids": self.encodings["input_ids"][idx],
            "attention_mask": self.encodings["attention_mask"][idx],
            "labels": self.labels[idx]
        }

# Create Dataset objects
train_dataset = TextDataset(train_encodings, y_train_tensor)
test_dataset = TextDataset(test_encodings, y_test_tensor)

BATCH_SIZE = 16
# DataLoader
train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)
test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)

class PrivacyAccountant:
    """Privacy Accountant for DP-SGD training."""

    def __init__(self, target_delta: float, noise_multiplier: float,
                 sampling_rate: float, max_steps: int):
        """
        Initialize the privacy accountant.

        Args:
            target_delta: Target delta value for (ε, δ)-DP.
            noise_multiplier: Noise multiplier (σ) used in DP-SGD.
            sampling_rate: Sampling rate (batch_size / dataset_size).
            max_steps: Maximum number of training steps.
        """
        self.target_delta = target_delta
        self.noise_multiplier = noise_multiplier
        self.sampling_rate = sampling_rate
        self.max_steps = max_steps
        self.steps = 0

    def get_privacy_spent(self, step: int = None) -> tuple[float, float]:
        """
        Calculate the privacy spent (ε) after a given number of steps.

        Args:
            step: The step to calculate privacy for (if None, use current step).

        Returns:
            Tuple of (epsilon, delta).
        """
        if step is None:
            step = self.steps

        # Use RDP accounting method
        # For simplicity, we'll use a basic calculation approach
        # In practice, more sophisticated methods should be used

        # This is a simplified version based on moments accountant
        c = self.sampling_rate * step
        epsilon = c * self.sampling_rate**2 / (self.noise_multiplier**2)

        # Convert from RDP to (ε, δ)-DP
        # This is a simplified approximation
        epsilon = epsilon + 2 * math.sqrt(epsilon * math.log(1/self.target_delta))

        return epsilon, self.target_delta

    def step(self):
        """Increment the step counter."""
        self.steps += 1

    def get_current_epsilon(self) -> float:
        """Get current epsilon value."""
        epsilon, _ = self.get_privacy_spent()
        return epsilon

class PhantomGradientClipper:
    """Phantom gradient clipper for DP-SGD."""

    def __init__(self, max_norm: float):
        """
        Initialize the phantom gradient clipper.

        Args:
            max_norm: The maximum norm for gradient clipping.
        """
        self.max_norm = max_norm

    def clip_gradients(self, model: nn.Module) -> tuple[list[torch.Tensor], float]:
        """
        Clip gradients using the phantom clipping method.

        Args:
            model: The model with gradients to clip.

        Returns:
            Tuple of (clipped_gradients, scaling_factor)
        """
        # Collect all gradients
        all_grads = []
        for param in model.parameters():
            if param.requires_grad and param.grad is not None:
                all_grads.append(param.grad.view(-1))

        # Concatenate all gradients into a single vector
        grad_vec = torch.cat(all_grads)

        # Calculate the global norm
        global_norm = torch.norm(grad_vec, p=2)

        # Calculate the scaling factor
        scaling_factor = min(1.0, self.max_norm / (global_norm + 1e-12))

        # Phantom clipping: instead of modifying the gradients in-place,
        # return the scaling factor and the clipped gradients
        clipped_grads = []
        i = 0
        for param in model.parameters():
            if param.requires_grad and param.grad is not None:
                numel = param.grad.numel()
                clipped_grads.append(param.grad.clone() * scaling_factor)
                i += numel

        return clipped_grads, scaling_factor

class DPSGDOptimizer(Optimizer):
    """Custom DPSGD optimizer with phantom clipping."""

    def __init__(self, params, lr=0.01, noise_multiplier=1.0, max_grad_norm=1.0,
                 weight_decay=0, momentum=0):
        """
        Initialize the DP-SGD optimizer.

        Args:
            params: Model parameters to optimize.
            lr: Learning rate.
            noise_multiplier: Noise multiplier (σ) for DP-SGD.
            max_grad_norm: Maximum gradient norm for clipping.
            weight_decay: Weight decay factor.
            momentum: Momentum factor.
        """
        defaults = dict(lr=lr, noise_multiplier=noise_multiplier,
                       max_grad_norm=max_grad_norm, weight_decay=weight_decay,
                       momentum=momentum)
        super(DPSGDOptimizer, self).__init__(params, defaults)

        self.clipper = PhantomGradientClipper(max_grad_norm)

    @torch.no_grad()
    def step(self, closure=None):
        """Perform a single optimization step with DP-SGD."""
        loss = None
        if closure is not None:
            with torch.enable_grad():
                loss = closure()

        # Get the first param group (assumes all params are in the same group)
        group = self.param_groups[0]

        # Get parameters from group
        noise_multiplier = group['noise_multiplier']
        max_grad_norm = group['max_grad_norm']
        lr = group['lr']
        weight_decay = group['weight_decay']
        momentum = group['momentum']

        # Get model from first parameter that has gradients
        param_model = None
        for p in group['params']:
            if p.grad is not None:
                param_model = p
                break

        if param_model is None:
            return loss

        # Apply phantom clipping to all parameters with gradients
        clipped_grads, scaling_factor = self.clipper.clip_gradients(param_model.grad.device)

        # Apply weight decay before adding noise (if needed)
        if weight_decay != 0:
            for p in group['params']:
                if p.grad is not None:
                    p.grad.add_(p.data, alpha=weight_decay)

        # Add DP noise to the gradients and update
        params_with_grad = [p for p in group['params'] if p.grad is not None]

        for i, (p, clipped_grad) in enumerate(zip(params_with_grad, clipped_grads)):
            # Generate noise
            noise = torch.randn_like(p.grad) * noise_multiplier * max_grad_norm

            # Apply noisy gradient
            if momentum != 0:
                # Apply momentum with noisy gradient
                param_state = self.state[p]
                if 'momentum_buffer' not in param_state:
                    param_state['momentum_buffer'] = torch.clone(clipped_grad).detach() + noise
                else:
                    param_state['momentum_buffer'].mul_(momentum).add_(clipped_grad + noise, alpha=1 - momentum)

                p.data.add_(param_state['momentum_buffer'], alpha=-lr)
            else:
                # Standard SGD update with noisy gradient
                p.data.add_(clipped_grad + noise, alpha=-lr)

        return loss

class ReAttentionBertEncoder(nn.Module):
    """BERT Encoder with Re-Attention Mechanism."""

    def __init__(self, config, dp_sigma):
        """
        Initialize the Re-Attention BERT Encoder.

        Args:
            config: BERT configuration object.
            dp_sigma: The DP noise level (σ).
        """
        super(ReAttentionBertEncoder, self).__init__()

        # Load pre-trained BERT model
        self.bert = BertModel.from_pretrained('bert-base-uncased')
        self.config = config
        self.dp_sigma = dp_sigma

        # Initialize the variance tracking parameters (c, d) for each layer
        self.variance_params = nn.ParameterList([
            nn.Parameter(torch.zeros(2)) for _ in range(config.num_hidden_layers)
        ])

        # Set the variance params to starting values
        for params in self.variance_params:
            # Initialize c and d for each layer
            params.data[0] = 0.0  # c
            params.data[1] = 1.0  # d

        # Modify the attention to use re-attention
        self._inject_reattention()

    def _inject_reattention(self):
        """Inject Re-Attention mechanism into BERT model."""
        # Save the original forward method for the first attention layer
        # to use as a template for all layers
        original_attn_forward = self.bert.encoder.layer[0].attention.self.forward

        def reattention_forward(self_, query, key, value, attention_mask=None, head_mask=None, output_attentions=False):
            """Forward pass with Re-Attention mechanism."""
            # Original attention calculation
            mixed_query_layer = self_.query(query)
            mixed_key_layer = self_.key(key)
            mixed_value_layer = self_.value(value)

            query_layer = self_.transpose_for_scores(mixed_query_layer)
            key_layer = self_.transpose_for_scores(mixed_key_layer)
            value_layer = self_.transpose_for_scores(mixed_value_layer)

            # Scaled dot-product attention
            attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))
            attention_scores = attention_scores / math.sqrt(self_.attention_head_size)

            if attention_mask is not None:
                attention_scores = attention_scores + attention_mask

            # Get the corresponding variance parameters for this layer
            layer_idx = int(self_._get_layer_idx())
            c, d = self.variance_params[layer_idx]

            # Transform c, d to get sigma for this layer
            sigma = torch.exp(c) * (d ** 2)

            # Apply the Re-Attention correction factor: S / exp[C*σ²/2]
            correction_factor = torch.exp(self_.C * (sigma ** 2) / 2.0)
            attention_scores = attention_scores / correction_factor

            # Continue with normal attention
            attention_probs = nn.functional.softmax(attention_scores, dim=-1)
            attention_probs = self_.dropout(attention_probs)

            if head_mask is not None:
                attention_probs = attention_probs * head_mask

            context_layer = torch.matmul(attention_probs, value_layer)
            context_layer = context_layer.permute(0, 2, 1, 3).contiguous()
            new_context_layer_shape = context_layer.size()[:-2] + (self_.all_head_size,)
            context_layer = context_layer.view(new_context_layer_shape)

            outputs = (context_layer, attention_probs) if output_attentions else (context_layer,)
            return outputs

        # Add the layer index method to attention layers
        for i, layer in enumerate(self.bert.encoder.layer):
            layer.attention.self._get_layer_idx = lambda idx=i: idx
            layer.attention.self.C = 1.0  # Constant for the correction factor

            # Replace the forward method with our re-attention version
            layer.attention.self.forward = types.MethodType(reattention_forward, layer.attention.self)

    def update_variance_params(self):
        """Update the variance parameters after each batch."""
        # This function would be called after each optimization step
        # It updates the (c, d) parameters based on the F function
        for i, params in enumerate(self.variance_params):
            c, d = params.data

            # Simple update function (F in the algorithm)
            # In practice, this should be calibrated based on empirical analysis
            new_c = c * 0.99  # Decay factor
            new_d = d * 0.99 + 0.01 * self.dp_sigma  # Blend with current DP noise level

            params.data[0] = new_c
            params.data[1] = new_d

    def forward(self, input_ids, attention_mask=None, token_type_ids=None):
        """Forward pass of the Re-Attention BERT model."""
        outputs = self.bert(
            input_ids=input_ids,
            attention_mask=attention_mask,
            token_type_ids=token_type_ids,
            output_hidden_states=True
        )

        # Update variance parameters
        self.update_variance_params()

        return outputs

class BertReAttentionForSequenceClassification(nn.Module):
    """BERT with Re-Attention for sequence classification."""

    def __init__(self, num_labels, dp_sigma, apply_lora=True):
        """
        Initialize the model.

        Args:
            num_labels: Number of classification labels.
            dp_sigma: The DP noise level.
            apply_lora: Whether to apply LoRA for parameter-efficient fine-tuning.
        """
        super(BertReAttentionForSequenceClassification, self).__init__()

        # Initialize BERT configuration
        self.config = BertConfig.from_pretrained('bert-base-uncased')
        self.config.num_labels = num_labels

        # Initialize the Re-Attention BERT encoder
        self.encoder = ReAttentionBertEncoder(self.config, dp_sigma)

        # Classification head
        self.dropout = nn.Dropout(self.config.hidden_dropout_prob)
        self.classifier = nn.Linear(self.config.hidden_size, num_labels)

        # Initialize classifier weights
        self._init_weights(self.classifier)

        # Apply LoRA if requested
        self.apply_lora = apply_lora
        if apply_lora:
            # Create a LoRA configuration
            self.lora_config = LoraConfig(
                r=8,  # Rank
                lora_alpha=32,
                target_modules=["query", "value"],  # BERT attention modules to modify
                lora_dropout=0.1,
                bias="none",
                task_type=TaskType.SEQ_CLS
            )

            # Apply LoRA to the model
            # We'll do this after the model is fully initialized

    def _init_weights(self, module):
        """Initialize the weights."""
        if isinstance(module, nn.Linear):
            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)
            if module.bias is not None:
                module.bias.data.zero_()

    def forward(self, input_ids, attention_mask=None, token_type_ids=None, labels=None):
        """
        Forward pass of the classification model.

        Args:
            input_ids: Token ids.
            attention_mask: Attention mask.
            token_type_ids: Token type ids.
            labels: Optional ground truth labels.

        Returns:
            Dict containing loss and logits.
        """
        outputs = self.encoder(
            input_ids=input_ids,
            attention_mask=attention_mask,
            token_type_ids=token_type_ids
        )

        # Get the [CLS] token representation (pooled output)
        pooled_output = outputs[1]

        # Apply dropout and classification layer
        pooled_output = self.dropout(pooled_output)
        logits = self.classifier(pooled_output)

        # Prepare the outputs tuple
        model_outputs = (logits,) + outputs[2:]  # Add hidden states if present

        # Calculate loss if labels are provided
        loss = None
        if labels is not None:
            loss_fct = nn.CrossEntropyLoss()
            loss = loss_fct(logits.view(-1, self.config.num_labels), labels.view(-1))
            model_outputs = (loss,) + model_outputs

        # Create a structure similar to HuggingFace's output
        class ModelOutput:
            def __init__(self, loss=None, logits=None, hidden_states=None):
                self.loss = loss
                self.logits = logits
                self.hidden_states = hidden_states

        if loss is not None:
            return ModelOutput(loss=model_outputs[0], logits=model_outputs[1])
        else:
            return ModelOutput(logits=model_outputs[0])

model = BertForSequenceClassification.from_pretrained(
    'bert-base-uncased',
    num_labels=5  # 5 class labels
)

total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
print(f"Total parameters count: {total_params:,}")

lora_config = LoraConfig(
    r=8,  # Rank
    lora_alpha=32,
    target_modules=["query", "value"],  # BERT attention modules to modify
    lora_dropout=0.1,
    bias="none",
    task_type=TaskType.SEQ_CLS
)

model = get_peft_model(model, lora_config)
trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
print(f"Total trainable parameters with LoRA: {trainable_params:,}")

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
model.to(device)
print("Available Device: ", device)

loss_fn = torch.nn.CrossEntropyLoss()

optimizer = AdamW(model.parameters(), lr=2e-5)
EPOCHS = 3

privacy_engine = PrivacyEngine()
EPSILON = 7.5
DELTA = 1e-5
MAX_GRAD_NORM = 1.0

model, optimizer, train_loader = privacy_engine.make_private_with_epsilon(
    module=model,
    optimizer=optimizer,
    data_loader=train_loader,
    target_delta=DELTA,
    target_epsilon=EPSILON,
    epochs=EPOCHS,
    max_grad_norm=MAX_GRAD_NORM,
)

def evaluate_model(model, test_loader, device):
    model.eval()
    test_preds = []
    test_true = []

    with torch.no_grad():
        for batch in tqdm(test_loader, desc="Evaluating"):
            input_ids = batch['input_ids'].to(device)
            attention_mask = batch['attention_mask'].to(device)
            labels = batch['labels'].to(device)

            outputs = model(
                input_ids=input_ids,
                attention_mask=attention_mask
            )

            # Get predicted class (argmax)
            preds = torch.argmax(outputs.logits, dim=1)
            test_preds.extend(preds.cpu().numpy())
            test_true.extend(labels.cpu().numpy())

    test_accuracy = accuracy_score(test_true, test_preds)

    all_preds = np.array(test_preds)
    all_labels = np.array(test_true)

    all_labels_indices = np.argmax(all_labels, axis=1) if all_labels.ndim > 1 else all_labels
    all_preds_indices = np.argmax(all_preds, axis=1) if all_preds.ndim > 1 else all_preds

    f1 = f1_score(all_labels_indices, all_preds_indices, average='weighted')

    return test_accuracy, f1, test_true, test_preds

total_steps = len(train_loader) * EPOCHS
scheduler = get_linear_schedule_with_warmup(
    optimizer,
    num_warmup_steps=0,
    num_training_steps=total_steps
)

epsilon_values = []
accuracy_values = []
train_loss_values = []

for epoch in range(EPOCHS):
    model.train()
    train_losses = []
    total_loss = 0.0

    for batch in tqdm(train_loader, desc=f"Epoch {epoch+1}/{EPOCHS}"):
        optimizer.zero_grad()

        input_ids = batch['input_ids'].to(device)
        attention_mask = batch['attention_mask'].to(device)
        labels = batch['labels'].to(device)

        outputs = model(
            input_ids=input_ids,
            attention_mask=attention_mask
        )

        # Using CrossEntropyLoss
        loss = loss_fn(outputs.logits, labels)
        train_losses.append(loss.item())

        loss.backward()
        optimizer.step()

        total_loss += loss.item()

    scheduler.step()

    # Get current epsilon value from privacy engine
    current_epsilon = privacy_engine.get_epsilon(DELTA)
    epsilon_values.append(current_epsilon)

    avg_train_loss = total_loss / len(train_loader)
    train_loss_values.append(avg_train_loss)
    print(f"Training Loss: {avg_train_loss:.4f}")
    print(f"Current ε: {current_epsilon:.2f} (Target: {EPSILON:.2f})")

test_accuracy, f1, test_true, test_preds = evaluate_model(model, test_loader, device)
print("\nFinal Results:")
print(f"Test Accuracy: {test_accuracy:.4f}")
print(f"F1 Score: {f1:.4f}")
print(classification_report(test_true, test_preds))

conf_matrix = confusion_matrix(test_true, test_preds)
print("Confusion Matrix:\n", conf_matrix)

plt.figure(figsize=(12, 8))

# Plot epsilon values
plt.plot(range(1, EPOCHS + 1), epsilon_values, 'b-o', linewidth=2)
plt.axhline(y=EPSILON, color='r', linestyle='--', alpha=0.7, label=f'Target ε = {EPSILON}')
plt.xlabel('Epoch')
plt.ylabel('Privacy Budget (ε)')
plt.title('Privacy Budget Consumption Over Training')
plt.grid(True, alpha=0.3)
plt.legend()
plt.tight_layout()
plt.savefig('dp_epsilon.png')
plt.show()

plt.figure(figsize=(15, 10))
plt.subplot(2, 1, 1)
plt.plot(range(1, EPOCHS + 1), epsilon_values, 'b-o', linewidth=2)
plt.axhline(y=EPSILON, color='r', linestyle='--', alpha=0.7, label=f'Target ε = {EPSILON}')
plt.xlabel('Epoch')
plt.ylabel('Privacy Budget (ε)')
plt.title('Privacy Budget (ε) vs Epochs')
plt.grid(True, alpha=0.3)
plt.legend()

plt.subplot(2, 1, 2)
plt.plot(range(1, EPOCHS + 1), train_loss_values, 'r-o', linewidth=2)
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.title('Training Loss vs Epochs')
plt.grid(True, alpha=0.3)
plt.tight_layout()
plt.savefig('dp_training_detailed_metrics.png')
plt.show()

torch.save(model.state_dict(), 'bert_lora_privacy.pt')