# -*- coding: utf-8 -*-
"""robert-lora-privacy.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1YdAiptcMosclpn1AMqQIYwbYJ22jE6Yi
"""

pip install transformers datasets torch peft opacus

import torch
import torch.nn as nn
import torch.optim as optim
from transformers import RobertaTokenizer, RobertaForSequenceClassification, RobertaConfig
from transformers import get_linear_schedule_with_warmup
from torch.utils.data import DataLoader, Dataset
from torch.optim import AdamW
import math
from typing import List, Dict, Optional
import numpy as np
import pandas as pd
import nltk
from nltk.stem import PorterStemmer
from nltk.corpus import stopwords
from sklearn.metrics import accuracy_score, classification_report
from sklearn.model_selection import train_test_split
from tqdm import tqdm
from peft import get_peft_model, LoraConfig, TaskType
from sklearn.model_selection import KFold
from opacus import PrivacyEngine

splits = {'train': 'yelp_review_full/train-00000-of-00001.parquet', 'test': 'yelp_review_full/test-00000-of-00001.parquet'}
df = pd.read_parquet("hf://datasets/Yelp/yelp_review_full/" + splits["train"])
df.head()

df.isnull().sum()

df.dropna(inplace=True)
df = df.iloc[:100001,:]
df.shape

"""# Data Prep"""

# converting text to lower case
# df = df.copy()
df = df.applymap(lambda x: x.lower() if isinstance(x, str) else x)

# removing contractions
contractions = {
    "ain't": "is not", "aren't": "are not", "can't": "cannot", "couldn't": "could not", "didn't": "did not",
    "doesn't": "does not", "don't": "do not", "hadn't": "had not", "hasn't": "has not", "haven't": "have not",
    "he'd": "he would", "he'll": "he will", "he's": "he is", "how'd": "how did", "how'll": "how will",
    "how's": "how is", "I’d": "I would", "I'll": "I will", "I'm": "I am", "I've": "I have", "I’m": "I am",
    "isn't": "is not", "it'd": "it would", "it'll": "it will", "it's": "it is", "let's": "let us", "ma'am": "madam",
    "mightn't": "might not", "mustn't": "must not", "needn't": "need not", "shan't": "shall not", "she'd": "she would",
    "she'll": "she will", "she's": "she is", "shouldn't": "should not", "that'd": "that would", "that'll": "that will",
    "that's": "that is", "there'd": "there would", "there'll": "there will", "there's": "there is", "they'd": "they would",
    "they'll": "they will", "they're": "they are", "they've": "they have", "wasn't": "was not", "weren't": "were not",
    "what'll": "what will", "what's": "what is", "what've": "what have", "where'd": "where did", "where'll": "where will",
    "where's": "where is", "who'd": "who would", "who'll": "who will", "who's": "who is", "who've": "who have",
    "why'd": "why did", "why'll": "why will", "why's": "why is", "won't": "will not", "wouldn't": "would not",
}
def expand_contractions(text):
    # Check if the input is a float
    if isinstance(text, float):
        return text

    for contraction, expanded in contractions.items():
        text = text.replace(contraction, expanded)

    return text

# apply
df['text'] = df['text'].apply(expand_contractions)

# removing puntuations
df = df.replace(r'[^\w\s]', '', regex=True)

# stop words removal

nltk.download('stopwords')
stop = set(stopwords.words('english'))

# for all col
df['text'] = df['text'].apply(lambda x: ' '.join([word for word in str(x).split() if word not in stop]) if x is not None else '')

# stemming
nltk.download('punkt_tab')
stemmer = PorterStemmer()

def stem_text(text):
    if isinstance(text, str):
        words = nltk.word_tokenize(text)
        stemmed_words = [stemmer.stem(word) for word in words]
        return ' '.join(stemmed_words)


df['text'] = df['text'].apply(stem_text)

df.head(5)

df.shape

"""# BERT"""

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from transformers import BertTokenizer
from torch.utils.data import Dataset, DataLoader
import torch
from transformers import BertForSequenceClassification #AdamW
from torch.nn import CrossEntropyLoss

# Encoding the target variable 'Popularity'
le = LabelEncoder()
df['label'] = le.fit_transform(df['label'])

from sklearn.model_selection import train_test_split

# Split data (80% train, 20% test)
X_train, X_test, y_train, y_test = train_test_split(
    df['text'], df['label'], test_size=0.2, random_state=42
)

tokenizer = RobertaTokenizer.from_pretrained('roberta-base')

# Define a function to tokenize the text data
def tokenize_texts(texts, max_length=512):
    return tokenizer.batch_encode_plus(
        texts,
        max_length=max_length,
        padding='max_length',
        truncation=True,
        return_tensors='pt'  # Return PyTorch tensors
    )

# Tokenize the training and test data
train_encodings = tokenize_texts(X_train.tolist())
test_encodings = tokenize_texts(X_test.tolist())

torch.save(train_encodings, 'train_encodings.pt')
torch.save(test_encodings, 'test_encodings.pt')

train_encodings = torch.load('train_encodings.pt', weights_only=False)
test_encodings = torch.load('test_encodings.pt', weights_only=False)

y_train_tensor = torch.tensor(y_train.tolist(), dtype=torch.long)
y_test_tensor = torch.tensor(y_test.tolist(), dtype=torch.long)

class TextDataset(torch.utils.data.Dataset):
    def __init__(self, encodings, labels):
        self.encodings = encodings
        self.labels = labels

    def __len__(self):
        return len(self.labels)

    def __getitem__(self, idx):
        return {
            "input_ids": self.encodings["input_ids"][idx],
            "attention_mask": self.encodings["attention_mask"][idx],
            "labels": self.labels[idx]
        }

# Create Dataset objects
train_dataset = TextDataset(train_encodings, y_train_tensor)
test_dataset = TextDataset(test_encodings, y_test_tensor)

BATCH_SIZE = 16
# DataLoader
train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)
test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)

config = RobertaConfig.from_pretrained('roberta-base')
config.num_labels = 5  # 5 classes for multi-class classification
config.problem_type = "single_label_classification"  # This is the key setting for multi-class

# Initialize the model with this config
model = RobertaForSequenceClassification.from_pretrained(
    'roberta-base',
    config=config
)
total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
print(f"Total parameters count: {total_params:,}")

lora_config = LoraConfig(
    r=8,  # Rank
    lora_alpha=32,
    target_modules=["query", "value"],  # BERT attention modules to modify
    lora_dropout=0.1,
    bias="none",
    task_type=TaskType.SEQ_CLS
)

model = get_peft_model(model, lora_config)
trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
print(f"Total trainable parameters with LoRA: {trainable_params:,}")

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
model.to(device)
print("Available Device: ", device)

loss_fn = torch.nn.CrossEntropyLoss()

optimizer = AdamW(model.parameters(), lr=2e-5)
EPOCHS = 3

privacy_engine = PrivacyEngine()
EPSILON = 7.5
DELTA = 1e-5
MAX_GRAD_NORM = 1.0

model, optimizer, train_loader = privacy_engine.make_private_with_epsilon(
    module=model,
    optimizer=optimizer,
    data_loader=train_loader,
    target_delta=DELTA,
    target_epsilon=EPSILON,
    epochs=EPOCHS,
    max_grad_norm=MAX_GRAD_NORM,
)

epsilon_values = []
train_loss_values = []

for epoch in range(EPOCHS):
    model.train()
    train_losses = []
    total_loss = 0.0

    for batch in tqdm(train_loader):
        optimizer.zero_grad()

        input_ids = batch['input_ids'].to(device)
        attention_mask = batch['attention_mask'].to(device)
        labels = batch['labels'].to(device)

        outputs = model(
            input_ids=input_ids,
            attention_mask=attention_mask,
            labels=labels
        )

        # Using CrossEntropyLoss
        loss = outputs.loss
        train_losses.append(loss.item())

        loss.backward()
        optimizer.step()

        total_loss += loss.item()

    total_steps = len(train_loader) * EPOCHS
    scheduler = get_linear_schedule_with_warmup(optimizer,
                                             num_warmup_steps=0,
                                             num_training_steps=total_steps)
    scheduler.step()

    current_epsilon = privacy_engine.get_epsilon(DELTA)
    epsilon_values.append(current_epsilon)

    avg_train_loss = total_loss / len(train_loader)
    train_loss_values.append(avg_train_loss)
    print(f"Training Loss: {avg_train_loss:.4f}")
    print(f"Current ε: {current_epsilon:.2f} (Target: {EPSILON:.2f})")

def evaluate_model(model, test_loader, device):
    model.eval()
    test_preds = []
    test_true = []

    with torch.no_grad():
        for batch in tqdm(test_loader, desc="Evaluating"):
            input_ids = batch['input_ids'].to(device)
            attention_mask = batch['attention_mask'].to(device)
            labels = batch['labels'].to(device)

            outputs = model(
                input_ids=input_ids,
                attention_mask=attention_mask
            )

            # Get predicted class (argmax)
            preds = torch.argmax(outputs.logits, dim=1)
            test_preds.extend(preds.cpu().numpy())
            test_true.extend(labels.cpu().numpy())

    test_accuracy = accuracy_score(test_true, test_preds)

    all_preds = np.array(test_preds)
    all_labels = np.array(test_true)

    all_labels_indices = np.argmax(all_labels, axis=1) if all_labels.ndim > 1 else all_labels
    all_preds_indices = np.argmax(all_preds, axis=1) if all_preds.ndim > 1 else all_preds

    f1 = f1_score(all_labels_indices, all_preds_indices, average='weighted')

    return test_accuracy, f1, test_true, test_preds

test_accuracy, f1, test_true, test_preds = evaluate_model(model, test_loader, device)
print("\nFinal Results:")
print(f"Test Accuracy: {test_accuracy:.4f}")
print(f"F1 Score: {f1:.4f}")
print(classification_report(test_true, test_preds))

conf_matrix = confusion_matrix(test_true, test_preds)
print("Confusion Matrix:\n", conf_matrix)

from matplotlib import pyplot as plt
plt.figure(figsize=(12, 8))

# Plot epsilon values
plt.plot(range(1, EPOCHS + 1), epsilon_values, 'b-o', linewidth=2)
plt.axhline(y=EPSILON, color='r', linestyle='--', alpha=0.7, label=f'Target ε = {EPSILON}')
plt.xlabel('Epoch')
plt.ylabel('Privacy Budget (ε)')
plt.title('Privacy Budget Consumption Over Training')
plt.grid(True, alpha=0.3)
plt.legend()
plt.tight_layout()
plt.savefig('dp_roberta_epsilon.png')
plt.show()

plt.figure(figsize=(15, 10))
plt.subplot(2, 1, 1)
plt.plot(range(1, EPOCHS + 1), epsilon_values, 'b-o', linewidth=2)
plt.axhline(y=EPSILON, color='r', linestyle='--', alpha=0.7, label=f'Target ε = {EPSILON}')
plt.xlabel('Epoch')
plt.ylabel('Privacy Budget (ε)')
plt.title('Privacy Budget (ε) vs Epochs')
plt.grid(True, alpha=0.3)
plt.legend()

plt.subplot(2, 1, 2)
plt.plot(range(1, EPOCHS + 1), train_loss_values, 'r-o', linewidth=2)
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.title('Training Loss vs Epochs')
plt.grid(True, alpha=0.3)
plt.tight_layout()
plt.savefig('dp_training_roberta_detailed_metrics.png')
plt.show()

torch.save(model.state_dict(), 'roberta_lora_privacy.pt')